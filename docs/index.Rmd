---
title: "Zonage des décisions - IA et Droit"
author: "Michaël Benesty"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    theme: united
    highlight: tango
    code_folding: show
    fig_width: 8
    fig_height: 6 
    toc: true
    number_sections: true
    toc_float:
      collapsed: false
      smooth_scroll: true
---

L'objet du projet [IA & Droit](http://openlaw.fr/travaux/communs-numeriques/ia-droit-datasets-dapprentissage) porté par l'association [Open Law](http://openlaw.fr/) est de créer un jeu de données qui permette de zoner les décisions de justice des Cours d'appel Françaises.
Le présent document est une illustration d'une baseline de résultat pour ce jeu de données.

La tâche consiste à qualifier chaque paragraphe d'une décision de justice selon un [plan prédéfini](https://github.com/pommedeterresautee/iaetdroit/releases) établie par les membres d'Open Law et la Cour de cassation.  

Pour rappel le dépot des données et du code source du projet est [ici](https://github.com/pommedeterresautee/iaetdroit).  
En particulier, les données brutes et déjà parsées et sérialisées au format CSV sont [ici](https://github.com/pommedeterresautee/iaetdroit/releases).

Pour cet exercice, il n'y a pas eu de recherche des hyper-paramètres qui pourraient être facilement améliorés.  
Il est su de l'auteur de ce code que de simples modifications de ce code source (augmentation des epoch, des ngrams, etc.) permet de gagner de 1 à plusieurs points sur chaque tâche de classification.  
Il est su de l'auteur de ce code que de ne pas différencier test set et dev set, c'est foncièrement mal.  
C'est la raison pour laquelle il est important de **prendre les présents résultats comme indicatifs** de ce qui peut être fait.  

L'approche choisie est une classification multiclass avec [`fastrtext`](https://github.com/pommedeterresautee/fastrtext/).  
Les prédiction des types de chaque paragraphe (micro et macro) sont séparées de la prédiction de la partie concernée.

# Pré-traitements

## Chargement des librairies

```{R lib_loading}
library(data.table)
library(DT)
library(fastrtext)
library(stringi)
library(assertthat)
library(ggplot2)
set.seed(123)
``` 

## Lecture des données

```{R read_data}
dt <- fread(input = "./annotations-clean.csv", encoding = "UTF-8")
setnames(dt, c("types", "types_macro"), c("paragraph_type_micro", "paragraph_type_macro"))

print(head(dt))

```

Il y a **`r nrow(dt)`** paragraphes dans le jeu de données.

## Retrait des paragraphes doublons

Certains paragraphes sont doublement annotés pour calculer l'inter-agreement.  
Ils sont retirés.

```{R remove_duplicates}
dt <- local({
  duplicated_files <- dt[,.N,.(file, dir)][, .(duplicated = duplicated(file), file, dir)][duplicated == TRUE]
  dt_with_duplicated_info <- merge(dt, duplicated_files, all.x = TRUE)
  dt_with_duplicated_info[is.na(duplicated)]
})
```

## Comptage des types

Ce comptage est fait avant le retrait de certaines catégories et/ou compression de plusieurs types en 1.  
Il s'agit de donner un aperçu de la répartition des données brutes.

```{R display_raw_types}
datatable(dt[, .(nb_mots_moyen = round(mean(stri_count_words(text))), nb_decisions = .N), paragraph_type_micro][, `%` := round(100 * nb_decisions / sum(nb_decisions), 2)])
```

## Répartition de la difficulté d'annotation

Les annotateurs ont noté la difficulté d'annoter chaque décision.

```{R difficulties}
datatable(dt[, .(nb_paragraphes = .N), annotation_difficulty][, `%` := round(100 * nb_paragraphes / sum(nb_paragraphes), 2)])
```

Les décisions jugées difficiles à annoter sont conservées dans le jeu de données.  
Le retrait de ces décisions ne change pas de façon significative les résultats.

## Groupement de certains types

Certains micro types de paragraphes sont regroupés.  
Les paragraphes typés `n_a` sont conservés.  
Leur retrait améliore considérablement la qualité des prédictions.  
Il est possible qu'un certain nombre de paragraphes `n_a` ne devraient pas l'être.

```{R simplify_tags}
# remove paragraph type position
dt[, paragraph_type_micro_cleaned := stri_replace_all_regex(paragraph_type_micro, "-\\d+", "")]

# remove double labels due to numbers
make_unique_labels <- function(label) {
  paste(sort(unique(unlist(stri_split_fixed(label, pattern = " ")))), collapse = " ")
}

dt[, paragraph_type_micro_cleaned := sapply(paragraph_type_micro_cleaned, make_unique_labels)]

# rationalizing motifs and dispositifs
dt[, paragraph_type_micro_cleaned := ifelse(stri_detect_regex(paragraph_type_micro_cleaned, "^Motif"), ifelse(stri_detect_fixed(paragraph_type_micro_cleaned, "Motif_texte"), "Motif_texte", "Motif"), paragraph_type_micro_cleaned)]
dt[paragraph_type_micro_cleaned == "Dispositif-demandes_accessoires", paragraph_type_micro_cleaned := "Dispositif_demandes_accessoires"]
dt[paragraph_type_micro_cleaned == "Dispositif Dispositif-demandes_accessoires", paragraph_type_micro_cleaned := "Dispositif_demandes_accessoires"]
dt[paragraph_type_micro_cleaned == "Contenu_decision_attaquee Expose_litige", paragraph_type_micro_cleaned := "Contenu_decision_attaquee_Expose_litige"]
dt[paragraph_type_micro_cleaned == "Entete_appelant Entete_avocat", paragraph_type_micro_cleaned := "Entete_appelant_avec_avocat"]
dt[paragraph_type_micro_cleaned == "Entete_avocat Entete_intime", paragraph_type_micro_cleaned := "Entete_intime_avec_avocat"]
dt[, paragraph_type_micro_cleaned := stri_replace_all_regex(paragraph_type_micro_cleaned, "_intime|_appelant", "")]

dt[, position := as.numeric(seq(paragraph_type_micro_cleaned)) / length(paragraph_type_micro_cleaned), file]

dt[, intime := stri_detect_fixed(paragraph_type_micro, "_intime")]
dt[, appelant := stri_detect_fixed(paragraph_type_micro, "_appelant")]

# check that no paragraph are related to both types.
stopifnot(dt[, sum(intime & appelant)] == 0)

dt[, side := ifelse(intime | appelant, ifelse(appelant, "appelant", "intime"), "aucun")]

# Extract the first 20% and the last 20% of each decision
dt <- local({
  intro <- dt[position < 0.2, .(intro = paste(text, collapse = "\n")), file]
  merge(dt, intro, by = "file")
})
```

## Préparation des données

La transformation des paragraphes pour l'apprentissage consiste essentiellement à ajouter les paragraphes qui précèdent et suivent sous forme de contexte.  
Présentement, les 3 paragraphes précédents et suivant sont ajoutés. Pour permettre au modèle de les distinguer du paragraphe à prédire, un préfixe est ajouté à chaque mot du contexte. Cette méthode augmente les résultats de plus de 10 points en fonction des tâches.  

L'introduction de chaque décision (dans notre cas les 20 premiers % de chaque décision) renseigne en général sur la nature des parties et sa thématique, pour cette raison elle est aussi ajoutée au contexte de chaque paragraphe.

L'ajout de la position du paragraphe dans la décision (par tranche de 10%) ne semble pas aider la prédiction lorsque l'introduction est en contexte mais produit un effet lorsque l'introduction n'est pas ajoutée au contexte (+1/+2 points selon les taches).

```{R text_preprocessing}
add_prefix <- function(prefix, labels) {
  add_prefix_item <- function(label, prefix) {
    s <- stri_extract_all_boundaries(label, simplify = TRUE)
    paste0(prefix, s, collapse = " ")
  }
  
  sapply(labels, FUN = add_prefix_item, prefix = prefix, USE.NAMES = FALSE)
}

swipe_features <- function(file, text, nbr) {
  if (nbr > 0) {
    p <- paste0("previous_", nbr, "_")
    r <- add_prefix(p, c(rep("", nbr), head(text, -nbr)))
    f <- c(rep("", nbr), head(file, -nbr)) == file
    ifelse(f, r, "")
  } else {
    nbr <- abs(nbr)
    p <- paste0("next_", nbr, "_")
    r <- add_prefix(p, c(tail(text, -nbr), rep("", nbr)))
    f <- c(tail(file, -nbr), rep("", nbr)) == file
    ifelse(f, r, "")
  }
}

dt[, text := stri_replace_all_regex(tolower(text), pattern = "[:punct:]", replacement = " ")]
dt[, `:=`(features_without_label = paste(swipe_features(file, text, 3), swipe_features(file, text, 2), swipe_features(file, text, 1), text, swipe_features(file, text, -1), swipe_features(file, text, -2), swipe_features(file, text, -3)), features_intro = add_prefix("intro_", intro), features_position = paste0("position_paragraphe_", 10 * round(position, 1)))]

train_rows <- seq(0.8 * nrow(dt))
test_rows <- seq(max(train_rows) + 1, nrow(dt))
```

### Paragraphe avec contexte tel que vu par l'algorithme

```{R example}
# Original text, paragraphs 1 to 7
print(dt[1:7, text])

# Paragraph 4 with its context (as seen by fastrtext)
print(dt[4, features_without_label])
```

# Apprentissages

## Typage des paragraphes

### Micro typage

On essaye ci-dessous de deviner la nature du paragraphe.

```{R paragraph_micro_types_learning}
learn_predict <- function(features){
  temp_file_train <- tempfile()
  temp_file_model <- tempfile()
  writeLines(dt[train_rows, sample(get(features))], con = temp_file_train)
  execute(commands = c("supervised", "-input", temp_file_train, "-output", temp_file_model, "-dim", 10, "-lr", 1, "-epoch", 20, "-wordNgrams", 2, "-verbose", 0))
  model <- load_model(temp_file_model)
  predictions <- predict(model, sentences = dt[test_rows][, get(features)], simplify = TRUE)
  predicted_labels <- names(predictions)
  invisible(assert_that(length(test_rows) == length(predicted_labels)))
  predicted_labels
}

display_prediction_accuracy <- function(pred_of_label, label_to_pred){
    tab <- dt[test_rows, .(nb_mots_moyen = round(mean(stri_count_words(text))), nb_decisions = .N, accuracy = round(100 * mean(get(label_to_pred) == get(pred_of_label)), 2)), get(label_to_pred)]
    setnames(tab, old = "get", label_to_pred)
    datatable(tab[order(-accuracy)])
}

dt[, features_with_type_label := paste(add_prefix("__label__", paragraph_type_micro_cleaned), features_without_label, features_intro, features_position)]
dt[test_rows, predicted_paragraph_micro := learn_predict(features = "features_with_type_label")]
display_prediction_accuracy(pred_of_label = "predicted_paragraph_micro", label_to_pred = "paragraph_type_micro_cleaned")
```

En moyenne, le bon type est trouvé dans **`r round(100 * dt[test_rows, mean(predicted_paragraph_micro == paragraph_type_micro_cleaned)], 2)`%** des **`r length(test_rows)`** paragraphes utilisés pour les tests.

Les erreurs se décomposent de la façon suivante :

```{R error_type_micro}
display_errors <- function(type_to_predict, prediction) {
  errors_dt <- dt[test_rows][, `to predict -> prediction` := paste(get(type_to_predict), "->", get(prediction))][get(type_to_predict) != get(prediction), .(nb_erreurs = .N), `to predict -> prediction`][, `%` := 100 * round(nb_erreurs / sum(nb_erreurs), 4)]
  datatable(errors_dt[order(-nb_erreurs)])
}
display_errors(type_to_predict = "paragraph_type_micro_cleaned", prediction = "predicted_paragraph_micro")
```

### Macro typage

Des tags macro sont présents dans le fichier.

```{R paragraph_paragraph_type_macro_learning}
dt[, features_with_type_macro := paste(add_prefix("__label__", paragraph_type_macro), features_without_label, features_intro, features_position)]
dt[test_rows, predicted_paragraph_macro := learn_predict(features = "features_with_type_macro")]
display_prediction_accuracy(pred_of_label = "predicted_paragraph_macro", label_to_pred = "paragraph_type_macro")
```

En moyenne, le bon type macro est trouvé dans **`r round(100 * dt[test_rows, mean(predicted_paragraph_macro == paragraph_type_macro)], 2)`%** des **`r length(test_rows)`** paragraphes utilisés pour les tests.

Les erreurs se décomposent de la façon suivante :

```{R error_type_macro}
  display_errors(type_to_predict = "paragraph_type_macro", prediction = "predicted_paragraph_macro")
```

## Partie concernée par un paragraphe

On essaye de prédire qui est lié au paragraphe.  
Le retrait de l'introduction et de la position semble aider (+5 points sur appelant et intimé).  
Cela impliquerait un overfit ?

```{R paragraph_side_learning}
dt[, features_with_side_label := paste(add_prefix("__label__", side), features_without_label)]
dt[test_rows, predicted_side := learn_predict(features = "features_with_side_label")]
display_prediction_accuracy(pred_of_label = "predicted_side", label_to_pred = "side")
```

En moyenne, la partie concernée par un paragraphe est trouvée dans **`r round(100 * dt[test_rows, mean(predicted_side == side)], 2)`%** des **`r length(test_rows)`** paragraphes utilisés pour les tests.

Les erreurs se décomposent de la façon suivante :

```{R error_side}
display_errors(type_to_predict = "side", prediction = "predicted_side")
```
